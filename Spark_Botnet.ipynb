{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Members: Kimiya Shabani, Quentin Mathieu, Christopher Steuer\n",
    "\n",
    "import findspark\n",
    " \n",
    "findspark.init()\n",
    " \n",
    "from pyspark import SparkContext,SparkConf\n",
    "import random\n",
    "import math\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import numpy\n",
    " \n",
    "conf = SparkConf().setAppName(\"assignment_botnet\")\n",
    "sc = SparkContext(\"local[*]\",\"assignment_botnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `readFile`\n",
    "\n",
    "#### Description\n",
    "The `readFile` function reads a dataset from a specified file and transforms it into an RDD (Resilient Distributed Dataset) for further processing with Apache Spark. The dataset is expected to have 12 columns, where the first 11 columns are features (X) and the 12th column is the label (Y).\n",
    "\n",
    "#### Arguments\n",
    "- `path` (string): The path to the dataset file.\n",
    "- `cols` (int, optional): The number of columns to read from the dataset file. Defaults to `np.inf`, which indicates reading all columns.\n",
    "\n",
    "#### Returns\n",
    "- `data_rdd` (RDD): An RDD containing the data from the specified file. Each record in the RDD is a tuple `(X, y)`, where:\n",
    "  - `X` (list of floats): An array containing the 11 features of an example.\n",
    "  - `y` (int): The label of the example, which is the 12th column (0 for normal traffic, 1 for botnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path,cols=np.inf):\n",
    "    rdd = sc.textFile(path)\n",
    "    data_rdd = rdd.map(lambda line: line.split(','))\n",
    "    data_rdd = data_rdd.map(lambda parts:(list(map(float, parts[:-1])),int(parts[-1])))\n",
    "    return data_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `normalize`\n",
    "\n",
    "### Description\n",
    "The `normalize` function is designed to normalize the features of a dataset represented as a Resilient Distributed Dataset (RDD) in Apache Spark. Normalization is a preprocessing step commonly used in machine learning to scale numeric features to a standard range, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1. This process ensures that each feature contributes equally to the analysis, preventing features with larger scales from dominating the learning algorithm.\n",
    "\n",
    "### Arguments\n",
    "- `rdd` (RDD): The RDD representing the dataset to be normalized.\n",
    "\n",
    "### Returns\n",
    "- `normalized_rdd` (RDD): An RDD containing the normalized data. Each record in the RDD is a tuple `(X_prime, y)`, where:\n",
    "  - `X_prime` (list of floats): An array containing the normalized feature values.\n",
    "  - `y` (int): The label (if applicable) associated with the example.\n",
    "\n",
    "### Steps\n",
    "1. **Extract Features and Calculate Aggregates:**\n",
    "   - The function starts by extracting the features from the RDD's first record. It then defines an inner function `extract_features` to process each data point. This function calculates the sum of feature values, the sum of squared feature values, and the count for each feature across all records in the RDD.\n",
    "   - These aggregates are computed using a `flatMap` transformation followed by a `reduceByKey` operation.\n",
    "\n",
    "2. **Calculate Mean and Standard Deviation:**\n",
    "   - After aggregating the feature values, the function computes the mean and standard deviation for each feature column. This is done by mapping over the aggregated RDD and applying formulas to calculate mean and standard deviation.\n",
    "\n",
    "3. **Normalize the Data:**\n",
    "   - Once the mean and standard deviation are calculated for each feature, the function normalizes each feature value in the dataset. This is achieved by subtracting the mean and dividing by the standard deviation for each feature, element-wise.\n",
    "   - The normalization is applied using a custom function `normalize_row` to each row of the RDD. The normalized RDD is then returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(rdd):\n",
    "    X = rdd.first()\n",
    "    \n",
    "    def extract_features(data):\n",
    "        X, y = data\n",
    "        features_with_counts_and_squares = []\n",
    "        for i in range(len(X)):\n",
    "            # Append the feature index, feature value, square of the feature value, and count\n",
    "            feature_tuple = (i, (X[i], X[i]**2, 1))\n",
    "            features_with_counts_and_squares.append(feature_tuple)\n",
    "        return features_with_counts_and_squares\n",
    "    \n",
    "    feature_sums = rdd.flatMap(extract_features)\n",
    "    # Reduce by key to sum the values, squared values, and counts for each feature across all records\n",
    "    feature_aggregates = feature_sums.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    " \n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    feature_stats = feature_aggregates.map(lambda x: (x[0], \n",
    "                                                      (x[1][0] / x[1][2],  # Mean\n",
    "                                                       math.sqrt((x[1][1] / x[1][2]) - (x[1][0] / x[1][2])**2)  # Std Dev\n",
    "                                                      )))\n",
    "    \n",
    "    sorted_feature_stats = feature_stats.sortByKey()\n",
    "    mean_elements= sorted_feature_stats.map(lambda x: x[1][0]).take(11)\n",
    "    std_elements= sorted_feature_stats.map(lambda x: x[1][1]).take(11)\n",
    "    \n",
    "    def normalize_row(row):\n",
    "        # Extract X from the row\n",
    "        X_row, y_row = row\n",
    "        # Calculate x_prime for each feature in the row\n",
    "        x_prime_row = [(X_row[i] - mean_elements[i]) / std_elements[i] for i in range(len(X_row))]\n",
    "        # Return the normalized row\n",
    "        return (x_prime_row, y_row)\n",
    "    # Apply the normalize_row function to each row of the RDD\n",
    "    normalized_rdd = rdd.map(normalize_row)\n",
    "    \n",
    "    return normalized_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `check_normalized`\n",
    "\n",
    "#### Description\n",
    "The `check_normalized` function is used to verify if the normalization of an RDD has been performed correctly. It calculates the mean and standard deviation for each feature in the dataset.\n",
    "\n",
    "#### Arguments\n",
    "- `rdd` (RDD): An RDD containing the normalized data. Each record in the RDD is a tuple `(X, y)`, where:\n",
    "  - `X` (list of floats): An array containing the features of an example.\n",
    "  - `y` (int): The label of the example.\n",
    "\n",
    "#### Returns\n",
    "- `feature_stats` (RDD): An RDD where each element is a tuple `(feature_index, (mean, std_dev))`, representing the mean and standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to see if the normalization worked correctly\n",
    "def check_normalized(rdd):\n",
    "    def extract_features(data):\n",
    "        X, y = data\n",
    "        features_with_counts_and_squares = []\n",
    "        for i in range(len(X)):\n",
    "            # Append the feature index, feature value, square of the feature value, and count\n",
    "            feature_tuple = (i, (X[i], X[i]**2, 1))\n",
    "            features_with_counts_and_squares.append(feature_tuple)\n",
    "        return features_with_counts_and_squares\n",
    "    feature_sums = rdd.flatMap(extract_features)\n",
    "    # Reduce by key to sum the values, squared values, and counts for each feature across all records\n",
    "    feature_aggregates = feature_sums.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    feature_stats = feature_aggregates.map(lambda x: (x[0], \n",
    "                                                      (x[1][0] / x[1][2],  # Mean\n",
    "                                                       math.sqrt((x[1][1] / x[1][2]) - (x[1][0] / x[1][2])**2)  # Std Dev\n",
    "                                                      )))\n",
    "    \n",
    "    return feature_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `train`\n",
    "\n",
    "#### Description\n",
    "The `train` function implements the Gradient Descent algorithm to optimize the weights of a logistic regression model using an RDD containing the dataset.\n",
    "\n",
    "#### Arguments\n",
    "- `rdd` (RDD): An RDD containing the training data. Each record in the RDD is a tuple `(X, y)`, where:\n",
    "  - `X` (list of floats): An array containing the features of an example.\n",
    "  - `y` (int): The label of the example.\n",
    "- `iterations` (int): The number of iterations to run the gradient descent algorithm.\n",
    "- `learning_rate` (float): The learning rate for the gradient descent algorithm.\n",
    "\n",
    "#### Returns\n",
    "- `w` (list of floats): The optimized weights (including bias) after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rdd,iterations,learning_rate):\n",
    "    #num_features = len(rdd.first()[0])\n",
    "    #print(num_features)\n",
    "    X = rdd.first()\n",
    " \n",
    "    m = len(X) #number of examples or rows in dataset\n",
    "    w =[]\n",
    "    # w includes b\n",
    "    num_columns = len(X[0])  # define the number of columns (and weights) to process\n",
    "    print (\"num_cols:\",num_columns)\n",
    "    for i in range(num_columns): \n",
    "        w.append(random.uniform(0.0, 1.0))\n",
    "    # init b with random\n",
    "    w.append(random.uniform(0.0, 1.0))\n",
    "    for i in range(iterations):\n",
    "        # Compute cost every 10 iterations for example            \n",
    "        cost = rdd.map(lambda x_y: fcost(x_y[1], get_y_hat(x_y[0], w))).mean()            \n",
    "        print(\"Iteration {}: Cost = {}\".format(i, cost)) \n",
    "        # Compute gradients for each data point        \n",
    "        gradients = rdd.map(lambda x_y: get_derivatives(x_y, w))                  \n",
    "        # Sum the gradients across all data points        \n",
    "        summed_gradients = gradients.reduce(lambda a, b: np.array(a) + np.array(b))                  \n",
    "        # Average the gradients        \n",
    "        mean_gradients = summed_gradients / rdd.count()         \n",
    "        # Update the weights       \n",
    "        w = update_ws(w, mean_gradients, learning_rate)   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_hat (x,w):\n",
    "    #print (x.shape, w.shape)\n",
    "    return sigmoid(get_dot_xw(x,w))\n",
    "\n",
    "def get_dot_xw(x,w):\n",
    "    \n",
    "    r = 0.\n",
    "    if (len(x)>len(w)):\n",
    "        print (\"getdot\",len(x),len(w))\n",
    "    for i in range(len(x)):\n",
    "        r += float(x[i])*float(w[i])\n",
    "    return r + w[-1]\n",
    "\n",
    "def sigmoid(x):\n",
    "    try:\n",
    "        return 1. / (1.+math.exp(-x))\n",
    "    except OverflowError:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function   \n",
    "def fcost(y, y_hat):\n",
    "    #print (\"cost:\",y,y_hat)\n",
    "    # compute loss/cost for one element \"y_hat\" and one label \"y\" \n",
    "    epsilon=0.00000001\n",
    "    if y == 1:\n",
    "        return -numpy.log(y_hat if y_hat > 0. else epsilon)\n",
    "    else:\n",
    "        return -numpy.log (1-y_hat if 1-y_hat >0. else epsilon)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model weights using current weights, their derivatives and the learning rate \n",
    "def update_ws(w,dw,lr):\n",
    "    w_ = numpy.array(w, dtype=float)\n",
    "    dw_ = numpy.array(dw, dtype=float)\n",
    "    #print (\"shapes w y dw\",w.shape,dw.shape)\n",
    "    tmp = w_ - lr*dw_\n",
    "    neww = tmp    \n",
    "    return neww\n",
    "\n",
    "# Get derivatives of Cost function for each element of the dataset\n",
    "def get_derivatives(x_y, w): \n",
    "    x = numpy.array(x_y[0], dtype=float)\n",
    "    y= x_y[1]\n",
    "    diff_y = get_y_hat(x,w) - y\n",
    "    # Vectorized version\n",
    "    # res = list(x*diff_y)+ [w[-1]]\n",
    "    res=[]\n",
    "    # dw\n",
    "    for x_i in x:\n",
    "        res.append(x_i*diff_y)\n",
    "    # db\n",
    "    res.append (w[-1])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (x,w):\n",
    "    threshold=0.5\n",
    "    y_hat=get_y_hat(x,w)\n",
    "    return 1 if y_hat > threshold else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `accuracy`\n",
    "\n",
    "#### Description\n",
    "The `accuracy` function calculates the accuracy of the logistic regression model on a given dataset.\n",
    "\n",
    "#### Arguments\n",
    "- `rdd_Xy` (RDD): An RDD containing the data examples. Each record of the RDD is a tuple `(X, y)`, where:\n",
    "  - `X` (list of floats): An array containing the features of an example.\n",
    "  - `y` (int): The label of the example.\n",
    "- `wf` (list of floats): The weights of the trained logistic regression model, including the bias term.\n",
    "\n",
    "#### Returns\n",
    "- `accuracy` (float): The accuracy of the model as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(rdd_Xy, wf): \n",
    "    # Compute the number of correct predictions   \n",
    "    correct_predictions = rdd_Xy.map(lambda x_y: (x_y[1], predict(x_y[0], wf))).map(lambda y_yhat: 1 if y_yhat[0] == y_yhat[1] else 0).reduce(lambda a, b: a + b)         \n",
    "    # Total number of examples    \n",
    "    total_examples = rdd_Xy.count()\n",
    "    # Calculate accuracy    \n",
    "    accuracy = (correct_predictions / total_examples) * 100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution parameters\n",
    "nIter = 20\n",
    "learningRate = 1.5\n",
    "path = \"/home/administrador/botnet_tot_syn_l.csv\"\n",
    " \n",
    "\n",
    "# read text_file in dataPath\n",
    "X_y = readFile(path)\n",
    " \n",
    "# Normalize\n",
    "X_y = normalize(X_y)\n",
    " \n",
    "# Train \n",
    "ws = train(X_y, nIter, learningRate)\n",
    "\n",
    "# Calculate the Accuracy\n",
    "acc = accuracy(X_y, ws)\n",
    "\n",
    "# Results\n",
    " \n",
    "print(\"Final weights of the model:\")\n",
    "print(ws)\n",
    "print(\" \")\n",
    "print(\"Accuracy of the model:\",acc,\"%\")\n",
    "print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
